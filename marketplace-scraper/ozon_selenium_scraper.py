# ozon_selenium_scraper.py
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium_stealth import stealth
import pandas as pd
import time
import re
import random
import os
import json
import logging
from datetime import datetime
from typing import Optional

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Ozon_Scraper")

def setup_driver():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–∞"""
    options = Options()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-extensions')
    options.add_argument('--start-maximized')
    options.add_argument(
        '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    )

    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    driver = webdriver.Chrome(options=options)

    stealth(driver,
            languages=["ru-RU", "ru"],
            vendor="Google Inc.",
            platform="Win32",
            webgl_vendor="Intel Inc.",
            renderer="Intel Iris OpenGL Engine",
            fix_hairline=True,
            )

    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: function() {return undefined;}})")

    return driver

def human_like_delay(min_seconds=1, max_seconds=3):
    """–°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞"""
    time.sleep(random.uniform(min_seconds, max_seconds))

def scroll_page(driver, max_scrolls=8):
    """–ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞"""
    logger.info("üìú –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞...")

    for scroll in range(max_scrolls):
        scroll_height = random.randint(800, 1200)
        driver.execute_script(f"window.scrollBy(0, {scroll_height});")
        logger.debug(f"üìú –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ {scroll + 1}/{max_scrolls}")

        human_like_delay(2, 3)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ –∫–æ–Ω—Ü–∞
        new_height = driver.execute_script("return document.body.scrollHeight")
        current_pos = driver.execute_script("return window.pageYOffset + window.innerHeight")

        if current_pos >= new_height - 100:
            logger.info("üõë –î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
            break

def find_all_products_safe(driver):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–æ–∏—Å–∫ –≤—Å–µ—Ö –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–æ–≤–∞—Ä–æ–≤ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
    logger.info("üîç –ò—â–µ–º –≤—Å–µ —Ç–æ–≤–∞—Ä—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ø–æ–∏—Å–∫–∞...")

    # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤
    try:
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div[class*='tile'], article[class*='tile']"))
        )
    except:
        logger.warning("‚è≥ –¢–æ–≤–∞—Ä—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –º–µ–¥–ª–µ–Ω–Ω–æ...")

    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å—Ä–∞–∑—É, –±–µ–∑ —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç—ã
    products_data = []
    seen_articles = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞—Ä—Ç–∏–∫—É–ª–æ–≤

    # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–æ–≤–∞—Ä–æ–≤
    selectors = [
        "div[class*='tile-root']",
        "article[class*='tile-root']",
        "div[class*='widget-search-result'] div[class*='tile']",
        "div[class*='search-result'] div[class*='tile']",
        "div[class*='tile']",
        "article[class*='tile']",
        "div[class*='card']"
    ]

    for selector in selectors:
        try:
            # –ö–∞–∂–¥—ã–π —Ä–∞–∑ –Ω–∞—Ö–æ–¥–∏–º —ç–ª–µ–º–µ–Ω—Ç—ã –∑–∞–Ω–æ–≤–æ
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for element in elements:
                try:
                    # –°—Ä–∞–∑—É –∏–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞
                    product_info = extract_product_info_immediately(element, driver)
                    if product_info:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É
                        if product_info['article'] not in seen_articles:
                            products_data.append(product_info)
                            seen_articles.add(product_info['article'])
                            logger.debug(f"üì¶ –î–æ–±–∞–≤–ª–µ–Ω —Ç–æ–≤–∞—Ä: {product_info['article']}")
                        else:
                            logger.debug(f"üîÑ –ü—Ä–æ–ø—É—â–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {product_info['article']}")
                except Exception as e:
                    continue

        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É {selector}: {e}")
            continue

    logger.info(f"üì¶ –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {len(products_data)}")
    return products_data

def extract_product_info_immediately(element, driver):
    """–ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Ç–æ–≤–∞—Ä
        link_selectors = [
            "a[href*='/product/']",
            "a[class*='tile-link']",
            "a[class*='card-link']"
        ]

        product_url = None
        for selector in link_selectors:
            try:
                link_element = element.find_element(By.CSS_SELECTOR, selector)
                product_url = link_element.get_attribute("href")
                if product_url:
                    break
            except:
                continue

        if not product_url:
            return None

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—Ä—Ç–∏–∫—É–ª –∏–∑ URL
        article = extract_article_from_url(product_url)
        if not article:
            return None

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
        name = extract_product_name(element)
        if not name:
            return None

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–Ω—É
        price = extract_accurate_price(element, driver)
        if not price or price < 1000:
            return None

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        rating = extract_rating(element)
        reviews_count = extract_reviews_count(element)
        old_price = extract_old_price(element)

        return {
            'marketplace': 'Ozon',
            'article': article,
            'name': name,
            'price': price,
            'old_price': old_price,
            'rating': rating,
            'reviews_count': reviews_count,
            'url': product_url,
            'collected_at': datetime.now().isoformat()
        }

    except Exception as e:
        return None

def extract_article_from_url(url):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ä—Ç–∏–∫—É–ª–∞ –∏–∑ URL —Ç–æ–≤–∞—Ä–∞"""
    patterns = [
        r'/product/[^/]*?(\d+)/',
        r'--(\d+)/?$',
        r'/(\d+)/?$'
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return ""

def extract_accurate_price(element, driver):
    """–¢–æ—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã"""
    try:
        price_selectors = [
            "span[class*='price']",
            "div[class*='price']",
            "span[class*='tsHeadline']",
            "div[class*='tsHeadline']",
            "span[class*='cost']",
            "div[class*='cost']",
            ".c311-a1", ".a0c1", ".a1v9",
            "[data-widget*='price']"
        ]

        for selector in price_selectors:
            try:
                price_elements = element.find_elements(By.CSS_SELECTOR, selector)
                for price_element in price_elements:
                    price_text = price_element.text.strip()
                    if price_text:
                        clean_text = re.sub(r'[^\d\s]', '', price_text)
                        clean_text = re.sub(r'\s+', '', clean_text)

                        if clean_text and len(clean_text) >= 3:
                            price = int(clean_text)
                            if 1000 <= price <= 100000:
                                return price
            except:
                continue

        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞ —Ü–µ–Ω—ã
        element_text = element.text
        price_patterns = [
            r'(\d{1,3}[ \‚ÄØ]?\d{3}[ \‚ÄØ]?\d{0,3})[ \‚ÄØ]?‚ÇΩ?',
            r'‚ÇΩ[ \‚ÄØ]*(\d{1,3}[ \‚ÄØ]?\d{3}[ \‚ÄØ]?\d{0,3})'
        ]

        for pattern in price_patterns:
            price_matches = re.findall(pattern, element_text)
            for match in price_matches:
                clean_price = re.sub(r'[^\d]', '', str(match))
                if clean_price:
                    price = int(clean_price)
                    if 1000 <= price <= 100000:
                        return price

        return None

    except Exception as e:
        return None

def extract_product_name(element):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞"""
    try:
        title_selectors = [
            "span[class*='tsBody']",
            "a[class*='title']",
            "span[class*='title']",
            "div[class*='title']",
            "h3", "h4",
            ".a5-a",
            "[class*='tile-title']"
        ]

        for selector in title_selectors:
            try:
                title_elements = element.find_elements(By.CSS_SELECTOR, selector)
                for title_element in title_elements:
                    title_text = title_element.text.strip()
                    if title_text and len(title_text) > 10:
                        return title_text
            except:
                continue

        # Fallback
        try:
            text = element.text.split('\n')[0]
            if text and len(text) > 10:
                return text
        except:
            pass

        return None

    except Exception as e:
        return None

def extract_rating(element):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–∞"""
    try:
        rating_selectors = [
            "span[class*='rating']",
            "div[class*='rating']",
            "[class*='star-rate']"
        ]

        for selector in rating_selectors:
            try:
                rating_elements = element.find_elements(By.CSS_SELECTOR, selector)
                for rating_elem in rating_elements:
                    rating_text = rating_elem.text.strip()
                    if rating_text:
                        match = re.search(r'(\d+[.,]\d+)', rating_text)
                        if match:
                            return float(match.group(1).replace(',', '.'))
            except:
                continue
        return None
    except:
        return None

def extract_reviews_count(element):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ—Ç–∑—ã–≤–æ–≤"""
    try:
        reviews_selectors = [
            "span[class*='review']",
            "div[class*='review']",
            "[class*='review-count']"
        ]

        for selector in reviews_selectors:
            try:
                reviews_elements = element.find_elements(By.CSS_SELECTOR, selector)
                for reviews_elem in reviews_elements:
                    reviews_text = reviews_elem.text.strip()
                    if reviews_text:
                        match = re.search(r'(\d+)', reviews_text)
                        if match:
                            return int(match.group(1))
            except:
                continue
        return 0
    except:
        return 0

def extract_old_price(element):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–π —Ü–µ–Ω—ã"""
    try:
        old_price_selectors = [
            "span[class*='old-price']",
            "div[class*='old-price']",
            "s[class*='price']"
        ]

        for selector in old_price_selectors:
            try:
                old_price_elements = element.find_elements(By.CSS_SELECTOR, selector)
                for old_elem in old_price_elements:
                    old_text = old_elem.text.strip()
                    if old_text:
                        clean_text = re.sub(r'[^\d\s]', '', old_text)
                        clean_text = re.sub(r'\s+', '', clean_text)

                        if clean_text and len(clean_text) >= 3:
                            old_price = int(clean_text)
                            if old_price > 1000:
                                return old_price
            except:
                continue
        return None
    except:
        return None

class OzonSeleniumScraper:
    def __init__(self, headless: bool = True):
        self.headless = headless

    def search_target_products(self, target_articles):
        """–ü–æ–∏—Å–∫ —Ü–µ–ª–µ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ –∞—Ä—Ç–∏–∫—É–ª–∞–º"""
        driver = setup_driver()
        found_products = []
        target_articles_set = set(target_articles)

        try:
            # URL –ø–æ–∏—Å–∫–∞ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
            search_url = "https://www.ozon.ru/search/?from_global=true&text=—É–º–Ω—ã–π+—Ç–µ–ª–µ–≤–∏–∑–æ—Ä+32+—Å+–≥–æ–ª–æ—Å–æ–≤—ã–º+—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º+os+salute+tv"

            logger.info(f"üåê –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞: {search_url}")
            driver.get(search_url)
            time.sleep(10)

            # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤
            scroll_page(driver, max_scrolls=10)

            # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –º–µ—Ç–æ–¥)
            all_products_data = find_all_products_safe(driver)

            logger.info(f"üîç –ò—â–µ–º —Ü–µ–ª–µ–≤—ã–µ –∞—Ä—Ç–∏–∫—É–ª—ã: {target_articles}")
            logger.info(f"üì¶ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(all_products_data)}")

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ü–µ–ª–µ–≤—ã–º –∞—Ä—Ç–∏–∫—É–ª–∞–º
            for product_data in all_products_data:
                if product_data['article'] in target_articles_set:
                    found_products.append(product_data)
                    logger.info(f"‚úÖ –ù–ê–ô–î–ï–ù –¶–ï–õ–ï–í–û–ô –¢–û–í–ê–†: {product_data['article']} - {product_data['name'][:50]}...")

            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
            found_products = self.remove_duplicates(found_products)

            logger.info(f"üìä –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ —Ü–µ–ª–µ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {len(found_products)}/{len(target_articles)}")

            # –ï—Å–ª–∏ –Ω–µ –≤—Å–µ –Ω–∞–π–¥–µ–Ω—ã, –≤—ã–≤–æ–¥–∏–º –∫–∞–∫–∏–µ –∏–º–µ–Ω–Ω–æ
            found_articles = {p['article'] for p in found_products}
            missing_articles = target_articles_set - found_articles
            if missing_articles:
                logger.warning(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –∞—Ä—Ç–∏–∫—É–ª—ã: {missing_articles}")

        except Exception as e:
            logger.error(f"üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
        finally:
            driver.quit()

        return found_products

    def remove_duplicates(self, products):
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ —Å–ø–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤"""
        if not products:
            return []

        logger.info("üßπ –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã...")
        logger.info(f"üìä –î–æ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(products)} —Ç–æ–≤–∞—Ä–æ–≤")

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–±)
        unique_products = []
        seen_articles = set()

        for product in products:
            article = product['article']
            if article not in seen_articles:
                unique_products.append(product)
                seen_articles.add(article)
            else:
                logger.debug(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {article}")

        logger.info(f"üìä –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(unique_products)} —Ç–æ–≤–∞—Ä–æ–≤")
        return unique_products

def save_to_json(data, filename="data/products_latest.json"):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON —Ñ–∞–π–ª –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ data"""
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é data –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        # –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –¥–ª—è latest —Ñ–∞–π–ª–∞
        existing_data = []
        if os.path.exists(filename) and "latest" in filename:
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            except json.JSONDecodeError:
                logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {filename} –ø–æ–≤—Ä–µ–∂–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π")
                existing_data = []
        
        # –î–ª—è —Ñ–∞–π–ª–æ–≤ —Å timestamp –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º
        if "latest" in filename:
            all_data = existing_data + data
        else:
            all_data = data
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É –∏ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—É
        unique_data = []
        seen = set()
        
        for item in all_data:
            key = (str(item['article']), item['marketplace'])
            if key not in seen:
                unique_data.append(item)
                seen.add(key)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(unique_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(data)} —Ç–æ–≤–∞—Ä–æ–≤ –≤ {filename} (–≤—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(unique_data)})")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ JSON: {e}")
        return False

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    scraper = OzonSeleniumScraper()
    
    # –ê—Ä—Ç–∏–∫—É–ª—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ (–¥–æ–ª–∂–Ω—ã –ø—Ä–∏—Ö–æ–¥–∏—Ç—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ —á–µ—Ä–µ–∑ main.py)
    target_articles = ["1955609657", "2573828081"]
    
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ Ozon")
    products = scraper.search_target_products(target_articles)
    
    if products:
        save_to_json(products)
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")
    else:
        logger.warning("‚ùå –¢–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
